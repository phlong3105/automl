{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "EfficientNetV2 TF2 with tf-hub",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYM61xrTsP5d"
   },
   "source": [
    "# EfficientNetV2 with tf-hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "<td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/google/automl/blob/master/efficientnetv2/tfhub.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on github\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.sandbox.google.com/github/google/automl/blob/master/efficientnetv2/tfhub.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "</td><td>\n",
    "    <!----<a href=\"https://tfhub.dev/google/collections/image/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />TF Hub models</a>--->\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1otmJgmbahf"
   },
   "source": [
    "## 1.Introduction\n",
    "\n",
    "[EfficientNetV2](https://arxiv.org/abs/2104.00298) is a family of classification models, with better accuracy, smaller size, and faster speed than previous models.\n",
    "\n",
    "\n",
    "This doc describes some examples with EfficientNetV2 tfhub. For more details, please visit the official code: https://github.com/google/automl/tree/master/efficientnetv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmaHHH7Pvmth"
   },
   "source": [
    "## 2.Select the TF2 SavedModel module to use"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FlsEcKVeuCnf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b13e40a4-132c-4fa6-cf0c-16723ad8bd06"
   },
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print('TF version:', tf.__version__)\n",
    "print('Hub version:', hub.__version__)\n",
    "print('Phsical devices:', tf.config.list_physical_devices())\n",
    "\n",
    "def get_hub_url_and_isize(model_name, ckpt_type, hub_type):\n",
    "  if ckpt_type == '1k':\n",
    "    ckpt_type = ''  # json doesn't support empty string\n",
    "  else:\n",
    "    ckpt_type = '-' + ckpt_type  # add '-' as prefix\n",
    "  \n",
    "  hub_url_map = {\n",
    "    'efficientnetv2-b0': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b0/{hub_type}',\n",
    "    'efficientnetv2-b1': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b1/{hub_type}',\n",
    "    'efficientnetv2-b2': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b2/{hub_type}',\n",
    "    'efficientnetv2-b3': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b3/{hub_type}',\n",
    "    'efficientnetv2-s':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-s/{hub_type}',\n",
    "    'efficientnetv2-m':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-m/{hub_type}',\n",
    "    'efficientnetv2-l':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-l/{hub_type}',\n",
    "\n",
    "    'efficientnetv2-b0-21k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b0-21k/{hub_type}',\n",
    "    'efficientnetv2-b1-21k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b1-21k/{hub_type}',\n",
    "    'efficientnetv2-b2-21k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b2-21k/{hub_type}',\n",
    "    'efficientnetv2-b3-21k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b3-21k/{hub_type}',\n",
    "    'efficientnetv2-s-21k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-s-21k/{hub_type}',\n",
    "    'efficientnetv2-m-21k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-m-21k/{hub_type}',\n",
    "    'efficientnetv2-l-21k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-l-21k/{hub_type}',\n",
    "    'efficientnetv2-xl-21k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-xl-21k/{hub_type}',\n",
    "\n",
    "    'efficientnetv2-b0-21k-ft1k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b0-21k-ft1k/{hub_type}',\n",
    "    'efficientnetv2-b1-21k-ft1k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b1-21k-ft1k/{hub_type}',\n",
    "    'efficientnetv2-b2-21k-ft1k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b2-21k-ft1k/{hub_type}',\n",
    "    'efficientnetv2-b3-21k-ft1k': f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-b3-21k-ft1k/{hub_type}',\n",
    "    'efficientnetv2-s-21k-ft1k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-s-21k-ft1k/{hub_type}',\n",
    "    'efficientnetv2-m-21k-ft1k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-m-21k-ft1k/{hub_type}',\n",
    "    'efficientnetv2-l-21k-ft1k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-l-21k-ft1k/{hub_type}',\n",
    "    'efficientnetv2-xl-21k-ft1k':  f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/efficientnetv2-xl-21k-ft1k/{hub_type}',\n",
    "      \n",
    "    # efficientnetv1\n",
    "    'efficientnet_b0': f'https://tfhub.dev/tensorflow/efficientnet/b0/{hub_type}/1',\n",
    "    'efficientnet_b1': f'https://tfhub.dev/tensorflow/efficientnet/b1/{hub_type}/1',\n",
    "    'efficientnet_b2': f'https://tfhub.dev/tensorflow/efficientnet/b2/{hub_type}/1',\n",
    "    'efficientnet_b3': f'https://tfhub.dev/tensorflow/efficientnet/b3/{hub_type}/1',\n",
    "    'efficientnet_b4': f'https://tfhub.dev/tensorflow/efficientnet/b4/{hub_type}/1',\n",
    "    'efficientnet_b5': f'https://tfhub.dev/tensorflow/efficientnet/b5/{hub_type}/1',\n",
    "    'efficientnet_b6': f'https://tfhub.dev/tensorflow/efficientnet/b6/{hub_type}/1',\n",
    "    'efficientnet_b7': f'https://tfhub.dev/tensorflow/efficientnet/b7/{hub_type}/1',\n",
    "  }\n",
    "  \n",
    "  image_size_map = {\n",
    "    'efficientnetv2-b0': 224,\n",
    "    'efficientnetv2-b1': 240,\n",
    "    'efficientnetv2-b2': 260,\n",
    "    'efficientnetv2-b3': 300,\n",
    "    'efficientnetv2-s':  384,\n",
    "    'efficientnetv2-m':  480,\n",
    "    'efficientnetv2-l':  480,\n",
    "    'efficientnetv2-xl':  512,\n",
    "  \n",
    "    'efficientnet_b0': 224,\n",
    "    'efficientnet_b1': 240,\n",
    "    'efficientnet_b2': 260,\n",
    "    'efficientnet_b3': 300,\n",
    "    'efficientnet_b4': 380,\n",
    "    'efficientnet_b5': 456,\n",
    "    'efficientnet_b6': 528,\n",
    "    'efficientnet_b7': 600,\n",
    "  }\n",
    "  \n",
    "  hub_url = hub_url_map.get(model_name + ckpt_type)\n",
    "  image_size = image_size_map.get(model_name, 224)\n",
    "  return hub_url, image_size\n",
    "\n",
    "\n",
    "def get_imagenet_labels(filename):\n",
    "  labels = []\n",
    "  with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "      labels.append(line.split('\\t')[1][:-1])  # split and remove line break.\n",
    "  return labels\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REiLDGq_W5FA"
   },
   "source": [
    "## 3.Inference with ImageNet 1k/2k checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9E65LgxQ39-X"
   },
   "source": [
    "## 3.1 ImageNet1k checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E32RGKBEWq76",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "outputId": "4d52e76c-b072-45ed-e8df-7a5e407c5816"
   },
   "source": [
    "# Build model\n",
    "import tensorflow_hub as hub\n",
    "model_name = 'efficientnetv2-s' #@param {type:'string'}\n",
    "ckpt_type = '1k'   # @param ['21k-ft1k', '1k']\n",
    "hub_type = 'classification' # @param ['classification', 'feature-vector']\n",
    "hub_url, image_size = get_hub_url_and_isize(model_name, ckpt_type, hub_type)\n",
    "tf.keras.backend.clear_session()\n",
    "m = hub.KerasLayer(hub_url, trainable=False)\n",
    "m.build([None, 224, 224, 3])  # Batch input shape.\n",
    "\n",
    "# Download label map file and image\n",
    "labels_map = '/tmp/imagenet1k_labels.txt'\n",
    "image_file = '/tmp/panda.jpg'\n",
    "tf.keras.utils.get_file(image_file, 'https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG')\n",
    "tf.keras.utils.get_file(labels_map, 'https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/v2/imagenet1k_labels.txt')\n",
    "\n",
    "# preprocess image.\n",
    "image = tf.keras.preprocessing.image.load_img(image_file, target_size=(224, 224))\n",
    "image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "image = (image - 128.) / 128.\n",
    "logits = m(tf.expand_dims(image, 0), False)\n",
    "\n",
    "# Output classes and probability\n",
    "pred = tf.keras.layers.Softmax()(logits)\n",
    "idx = tf.argsort(logits[0])[::-1][:5].numpy()\n",
    "classes = get_imagenet_labels(labels_map)\n",
    "for i, id in enumerate(idx):\n",
    "  print(f'top {i+1} ({pred[0][id]*100:.1f}%):  {classes[id]} ')\n",
    "from IPython import display\n",
    "display.display(display.Image(image_file))"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bB39DLaO5NGi"
   },
   "source": [
    "## 3.2 ImageNet21k checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o2EIhRmq5Mi0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "outputId": "772bbb62-b23d-44ae-85e6-e49186a32b14"
   },
   "source": [
    "# Build model\n",
    "import tensorflow_hub as hub\n",
    "model_name = 'efficientnetv2-s' #@param {type:'string'}\n",
    "ckpt_type = '21k'   # @param ['21k']\n",
    "hub_type = 'classification' # @param ['classification', 'feature-vector']\n",
    "hub_url, image_size = get_hub_url_and_isize(model_name, ckpt_type, hub_type)\n",
    "tf.keras.backend.clear_session()\n",
    "m = hub.KerasLayer(hub_url, trainable=False)\n",
    "m.build([None, 224, 224, 3])  # Batch input shape.\n",
    "\n",
    "# Download label map file and image\n",
    "labels_map = '/tmp/imagenet21k_labels.txt'\n",
    "image_file = '/tmp/panda2.jpeg'\n",
    "tf.keras.utils.get_file(image_file, 'https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG')\n",
    "tf.keras.utils.get_file(labels_map, 'https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/v2/imagenet21k_labels.txt')\n",
    "\n",
    "# preprocess image.\n",
    "image = tf.keras.preprocessing.image.load_img(image_file, target_size=(224, 224))\n",
    "image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "image = (image - 128.) / 128.\n",
    "logits = m(tf.expand_dims(image, 0), False)\n",
    "\n",
    "# Output classes and probability\n",
    "pred = tf.keras.activations.sigmoid(logits)  # 21k uses sigmoid for multi-label\n",
    "idx = tf.argsort(logits[0])[::-1][:20].numpy()\n",
    "classes = get_imagenet_labels(labels_map)\n",
    "for i, id in enumerate(idx):\n",
    "  print(f'top {i+1} ({pred[0][id]*100:.1f}%):  {classes[id]} ')\n",
    "  if pred[0][id] < 0.5:\n",
    "   break\n",
    "from IPython import display\n",
    "display.display(display.Image(image_file))"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTY8qzyYv3vl"
   },
   "source": [
    "## 4.Finetune with Flowers dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_12xwDuZFOQ"
   },
   "source": [
    "Get hub_url and image_size\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "50FYNIb1dmJH"
   },
   "source": [
    "# Build model\n",
    "import tensorflow_hub as hub\n",
    "model_name = 'efficientnetv2-b0' #@param {type:'string'}\n",
    "ckpt_type = '1k'   # @param ['21k', '21k-ft1k', '1k']\n",
    "hub_type = 'feature-vector' # @param ['feature-vector']\n",
    "batch_size =  32#@param {type:\"integer\"}\n",
    "hub_url, image_size = get_hub_url_and_isize(model_name, ckpt_type, hub_type)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lus9bIA-bQgj"
   },
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "umB5tswsfTEQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8df0490e-13cd-4de4-fa0a-a6551ecdb6d5"
   },
   "source": [
    "data_dir = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "    \n",
    "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
    "dataflow_kwargs = dict(target_size=(image_size, image_size),\n",
    "                       batch_size=batch_size,\n",
    "                       interpolation=\"bilinear\")\n",
    "\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **datagen_kwargs)\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\n",
    "\n",
    "do_data_augmentation = False #@param {type:\"boolean\"}\n",
    "if do_data_augmentation:\n",
    "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      horizontal_flip=True,\n",
    "      width_shift_range=0.2, height_shift_range=0.2,\n",
    "      shear_range=0.2, zoom_range=0.2,\n",
    "      **datagen_kwargs)\n",
    "else:\n",
    "  train_datagen = valid_datagen\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2e5WupIw2N2"
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9f3yBUvkd_VJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "99b79db6-d9f9-438e-9011-8221e8c5377c"
   },
   "source": [
    "# whether to finetune the whole model or just the top layer.\n",
    "do_fine_tuning = True #@param {type:\"boolean\"}\n",
    "num_epochs = 2 #@param {type:\"integer\"}\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=[image_size, image_size, 3]),\n",
    "    hub.KerasLayer(hub_url, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model.build((None, image_size, image_size, 3))\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    epochs=num_epochs, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=validation_steps).history"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oi1iCNB9K1Ai",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "outputId": "c8e2fc22-b4bd-4950-bce3-6c4af197f405"
   },
   "source": [
    "def get_class_string_from_index(index):\n",
    "   for class_string, class_index in valid_generator.class_indices.items():\n",
    "      if class_index == index:\n",
    "         return class_string\n",
    "\n",
    "x, y = next(valid_generator)\n",
    "image = x[0, :, :, :]\n",
    "true_index = np.argmax(y[0])\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
    "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(\"True label: \" + get_class_string_from_index(true_index))\n",
    "print(\"Predicted label: \" + get_class_string_from_index(predicted_index))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCsAsQM1IRvA"
   },
   "source": [
    "Finally, the trained model can be saved for deployment to TF Serving or TF Lite (on mobile) as follows."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LGvTi69oIc2d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "710b8803-3e34-4948-96e7-d905c5c96d64"
   },
   "source": [
    "saved_model_path = f\"/tmp/saved_flowers_model_{model_name}\"\n",
    "tf.saved_model.save(model, saved_model_path)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzW4oNRjILaq"
   },
   "source": [
    "## Optional: Deployment to TensorFlow Lite\n",
    "\n",
    "[TensorFlow Lite](https://www.tensorflow.org/lite) for mobile. Here we also runs tflite file in the TF Lite Interpreter to examine the resulting quality."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Va1Vo92fSyV6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "11956d83-1b22-4330-f57a-bfc893ec2b2a"
   },
   "source": [
    "optimize_lite_model = True  #@param {type:\"boolean\"}\n",
    "#@markdown Setting a value greater than zero enables quantization of neural network activations. A few dozen is already a useful amount.\n",
    "num_calibration_examples = 81  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "representative_dataset = None\n",
    "if optimize_lite_model and num_calibration_examples:\n",
    "  # Use a bounded number of training examples without labels for calibration.\n",
    "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
    "  representative_dataset = lambda: itertools.islice(\n",
    "      ([image[None, ...]] for batch, _ in train_generator for image in batch),\n",
    "      num_calibration_examples)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "if optimize_lite_model:\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  if representative_dataset:  # This is optional, see above.\n",
    "    converter.representative_dataset = representative_dataset\n",
    "lite_model_content = converter.convert()\n",
    "\n",
    "with open(f\"/tmp/lite_flowers_model_{model_name}.tflite\", \"wb\") as f:\n",
    "  f.write(lite_model_content)\n",
    "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
    "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_wqEmD0xIqeG"
   },
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
    "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
    "def lite_model(images):\n",
    "  interpreter.allocate_tensors()\n",
    "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
    "  interpreter.invoke()\n",
    "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JMMK-fZrKrk8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f5e205ee-42da-4041-b397-26408ad78b88"
   },
   "source": [
    "#@markdown For rapid experimentation, start with a moderate number of examples.\n",
    "num_eval_examples = 50  #@param {type:\"slider\", min:0, max:700}\n",
    "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
    "                for batch in train_generator\n",
    "                for (image, label) in zip(*batch))\n",
    "count = 0\n",
    "count_lite_tf_agree = 0\n",
    "count_lite_correct = 0\n",
    "for image, label in eval_dataset:\n",
    "  probs_lite = lite_model(image[None, ...])[0]\n",
    "  probs_tf = model(image[None, ...]).numpy()[0]\n",
    "  y_lite = np.argmax(probs_lite)\n",
    "  y_tf = np.argmax(probs_tf)\n",
    "  y_true = np.argmax(label)\n",
    "  count +=1\n",
    "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
    "  if y_lite == y_true: count_lite_correct += 1\n",
    "  if count >= num_eval_examples: break\n",
    "print(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
    "print(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
